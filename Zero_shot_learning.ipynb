{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import AdamW\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for CUDA availability:\n",
    "```python\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "```  \n",
    "This line checks if a CUDA-capable GPU is available. If it is, it sets the device variable to \"cuda\", otherwise it sets it to \"cpu\". This is important because using a GPU can significantly speed up the processing time compared to using the CPU.\n",
    "\n",
    "\n",
    "### Load CLIP model and preprocessing:\n",
    "```python\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "```\n",
    "This line loads the CLIP model (\"ViT-B/32\" version) and its associated preprocessing pipeline using the clip.load function. The device parameter ensures the model is loaded onto the appropriate device (GPU if available, otherwise CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the images\n",
    "super_directory = r'resources\\super'\n",
    "sadyek_directory = r'resources\\sadyek'\n",
    "\n",
    "# Load super images in the directory\n",
    "image_paths_super = [os.path.join(super_directory, file) for file in os.listdir(super_directory) if file.endswith('.bmp')]\n",
    "images_super = [preprocess(Image.open(image_path)).unsqueeze(0).to(device) for image_path in image_paths_super]\n",
    "# Load sadyek images in directory\n",
    "image_paths_sadyek = [os.path.join(sadyek_directory, file) for file in os.listdir(sadyek_directory) if file.endswith('.bmp')]\n",
    "images_sadyek = [preprocess(Image.open(image_path)).unsqueeze(0).to(device) for image_path in image_paths_sadyek]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine images into a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_tensor_super = torch.cat(images_super, dim=0)\n",
    "\n",
    "images_tensor_sadyek = torch.cat(images_sadyek, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = clip.tokenize(['A dried fig with some opening parts that darker,revealing its interior,deformed,yammi'\n",
    "                      , '  A dried fig have without opening parts,a little flat']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy super : 0.8316062176165803\n",
      "Accuracy Sadyek : 0.8337349397590361\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(images_tensor_super)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(images_tensor_super, text)\n",
    "    probs_super = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "\n",
    "probs_super = (probs_super>0.5)    \n",
    "Accuracy_super  = np.sum(probs_super[:,1])/len(probs_super)\n",
    "print(f'Accuracy super : {Accuracy_super}')\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(images_tensor_sadyek)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(images_tensor_sadyek, text)\n",
    "    probs_sadyek = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "    \n",
    "probs_sadyek = (probs_sadyek>0.5)\n",
    "Accuracy_sadyek = np.sum(probs_sadyek[:,0])/len(probs_sadyek)\n",
    "print(f'Accuracy Sadyek : {Accuracy_sadyek}')\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
